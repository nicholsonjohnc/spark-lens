{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collaborative filtering movie recommender system using the Movie Lens dataset and Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session and Spark Context\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark_lens\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the movie lens ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into an RDD.\n",
    "ratings_rdd = sc.textFile('data/u.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196\\t242\\t3\\t881250949',\n",
       " '186\\t302\\t3\\t891717742',\n",
       " '22\\t377\\t1\\t878887116',\n",
       " '244\\t51\\t2\\t880606923',\n",
       " '166\\t346\\t1\\t886397596',\n",
       " '298\\t474\\t4\\t884182806',\n",
       " '115\\t265\\t2\\t881171488',\n",
       " '253\\t465\\t5\\t891628467',\n",
       " '305\\t451\\t3\\t886324817',\n",
       " '6\\t86\\t3\\t883603013']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rows into fields.\n",
    "ratings_rdd = ratings_rdd.map(lambda row: row.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['196', '242', '3', '881250949'],\n",
       " ['186', '302', '3', '891717742'],\n",
       " ['22', '377', '1', '878887116'],\n",
       " ['244', '51', '2', '880606923'],\n",
       " ['166', '346', '1', '886397596'],\n",
       " ['298', '474', '4', '884182806'],\n",
       " ['115', '265', '2', '881171488'],\n",
       " ['253', '465', '5', '891628467'],\n",
       " ['305', '451', '3', '886324817'],\n",
       " ['6', '86', '3', '883603013']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast fields to types and drop timestamp.\n",
    "def cast(row):\n",
    "  user, movie, rating, timestamp = row\n",
    "  return (int(user), int(movie), float(rating))\n",
    "ratings_rdd = ratings_rdd.map(cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(196, 242, 3.0),\n",
       " (186, 302, 3.0),\n",
       " (22, 377, 1.0),\n",
       " (244, 51, 2.0),\n",
       " (166, 346, 1.0),\n",
       " (298, 474, 4.0),\n",
       " (115, 265, 2.0),\n",
       " (253, 465, 5.0),\n",
       " (305, 451, 3.0),\n",
       " (6, 86, 3.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a schema for our Spark DataFrame.\n",
    "schema = StructType([\n",
    "    StructField('user', IntegerType(), True),\n",
    "    StructField('movie', IntegerType(), True),\n",
    "    StructField('rating', DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Spark DataFrame.\n",
    "ratings_df = spark.createDataFrame(ratings_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|user|movie|rating|\n",
      "+----+-----+------+\n",
      "| 196|  242|   3.0|\n",
      "| 186|  302|   3.0|\n",
      "|  22|  377|   1.0|\n",
      "| 244|   51|   2.0|\n",
      "| 166|  346|   1.0|\n",
      "| 298|  474|   4.0|\n",
      "| 115|  265|   2.0|\n",
      "| 253|  465|   5.0|\n",
      "| 305|  451|   3.0|\n",
      "|   6|   86|   3.0|\n",
      "+----+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 10 rows.\n",
    "ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|              user|            movie|            rating|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|            100000|           100000|            100000|\n",
      "|   mean|         462.48475|        425.53013|           3.52986|\n",
      "| stddev|266.61442012750945|330.7983563255858|1.1256735991443179|\n",
      "|    min|                 1|                1|               1.0|\n",
      "|    max|               943|             1682|               5.0|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe DataFrame.\n",
    "ratings_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|nulls|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register temp table.\n",
    "ratings_df.registerTempTable('ratings')\n",
    "# Query table for number of nulls.\n",
    "spark.sql('''\n",
    "    SELECT COUNT(rating) AS nulls\n",
    "    FROM ratings \n",
    "    WHERE rating=null\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random 80/20 train/test split.\n",
    "train_df, test_df = ratings_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19837"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spark's implementation of an Alternating Least Squares (ALS) matrix factorization model.\n",
    "als_model  = ALS( \\\n",
    "    userCol='user', \\\n",
    "    itemCol='movie', \\\n",
    "    ratingCol='rating', \\\n",
    "    nonnegative=True, \\\n",
    "    regParam=0.1, \\\n",
    "    rank=10 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train recommender.\n",
    "recommender = als_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for entire test set.\n",
    "test_df = recommender.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+----------+\n",
      "|user|movie|rating|prediction|\n",
      "+----+-----+------+----------+\n",
      "| 633|  148|   1.0| 3.4631267|\n",
      "| 224|  148|   3.0| 3.3561032|\n",
      "| 328|  148|   3.0| 3.1503906|\n",
      "| 120|  148|   3.0| 2.7938714|\n",
      "|  92|  148|   2.0| 2.7115803|\n",
      "| 374|  148|   4.0| 3.2940223|\n",
      "| 834|  148|   4.0| 3.1915097|\n",
      "|  59|  148|   3.0|  2.879704|\n",
      "| 244|  148|   2.0|  2.708571|\n",
      "| 706|  148|   4.0| 3.4192502|\n",
      "+----+-----+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd = test_df.toPandas()\n",
    "test_pd = test_df.toPandas().fillna(test_pd['rating'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd['squared_error'] = (test_pd['rating'] - test_pd['prediction'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920786933145924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE\n",
    "np.sqrt(sum(test_pd['squared_error']) / len(test_pd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
